#!/bin/bash


 
#### Resource Request: ####
# All DragonsTooth nodes have the following hardware: 24 cores (2 x Intel Xeon E5-2680v3, Haswell), 256GB 2133 MHz DDR4
#
# Resources can be requested by specifying the number of nodes, cores, memory, etc
# Examples:
#   Request 2 nodes with 24 cores each
#   #PBS -l nodes=1:ppn=24
#   Request 4 cores (on any number of nodes)
#   #PBS -l procs=4
#   Request 12 cores with 20gb memory per core
#   #PBS -l procs=12,pmem=20gb
#   Request 2 nodes with 24 cores each and 20gb memory per core (will give two 512gb nodes)
#   #PBS -l nodes=2:ppn=24,pmem=20gb
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=32  
##SBATCH --ntasks=24


#### Walltime ####
# Set the walltime, which is the maximum time your job can run in HH:MM:SS
# Note that if your job exceeds the walltime estimated during submission, the scheduler
# will kill it. So it is important to be conservative (i.e., to err on the high side)
# with the walltime that you include in your submission script. 
#SBATCH -t 30:00:00

#### Queue ####
# Queue name. NewRiver has four queues:
#   normal_q   for production jobs on all Haswell nodes
#   dev_q      for development/debugging jobs. These jobs must be short but can be large.
#   open_q     for jobs that don't need an allocation
#SBATCH -p normal_q

#### Account ####
# This determines which allocation this job's CPU hours are billed to.
# Replace "youraccount" below with the name of your allocation account.
# If you are a student, you will need to get this from your advisor.
# For more on allocations, go here: http://www.arc.vt.edu/allocations
#SBATCH -A TSIMP_SLR

# Access group. Do not change this line.
#PBS -W group_list=dragonstooth

# Uncomment and add your email address to get an email when your job starts, completes, or aborts
##PBS -M weiszr@vt.edu
##PBS -m bea

# Add any modules you might require. This example removes all modules and then adds
# the Intel compiler and MKL scientific libraries. Use the module avail command
# to see a list of available modules.
#bash

# Change to the directory from which the job was submitted
cd $SLURM_SUBMIT_DIR

# Below here enter the commands to start your job. A few examples are provided below.
# Some useful variables set by the job:
#  $PBS_O_WORKDIR    Directory from which the job was submitted
#  $PBS_NODEFILE     File containing list of cores available to the job
#  $PBS_JOBID        Job ID (e.g., 111111.dt-scheduler.dt)
#  $PBS_NP           Number of cores allocated to the job
# Some useful storage locations (see ARC's Storage documentation for details): 
#  $HOME     Home directory. Use for permanent files.
#  $WORK     Work directory. Use for fast I/O.
#  $TMPFS    File system set up in memory for this job. Use for very fast, small I/O
#  $TMPDIR   Local disk (hard drive) space set up for this job

# Say "Hello world!"
#echo "Hello worldology-fortify-place-chancel!" 
##make clobber
export FFLAGS='-O2 -fopenmp'
make new
export OMP_NUM_THREADS=32
export OMP_STACKSIZE=16M
make .output
make fgmax_plots
make plots
#interact -N1 --exclusive #--ntasks-per-node=24
echo 'Hello World'
#python para_gc_runs.py -s RCP85K14 -n 100
##cd case7Runsa/
# Run the program a.out
##./a.out

# Run the OpenMP program ompProg with 24 threads (there are 24 cores per NewRiver Haswell node)
##export OMP_NUM_THREADS=48
#export OMP_NUM_THREADS=$PBS_NP 
##./ompProg
##nohup sh CPUHO.sh

#python bedload_options.py -m p -p 24 -f 1.5 -d -1.0 -g 24 -n m1
# Run the MPI program mpiProg. The -np flag tells MPI how many processes to use. $PBS_NP
# is an environment variable that holds the number of processes you requested. So if you
# selected nodes=2:ppn=24 above, $pbs_np will hold 48.
##mpirun -np $PBS_NP ./mpiProg

exit;
